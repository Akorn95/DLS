{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "DLS simpson.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQZnw3zpT8J2",
        "colab_type": "text"
      },
      "source": [
        "Корниевская Анастасия\n",
        "\n",
        "результаты, были получены запуском текущего нотебука на сервисах kaggle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "eimf4qG_OUcS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "from skimage import io\n",
        "import PIL\n",
        "import random\n",
        "\n",
        "from tqdm import tqdm, tqdm_notebook\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from multiprocessing.pool import ThreadPool\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "from matplotlib import colors, pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# в sklearn не все гладко, чтобы в colab удобно выводить картинки \n",
        "# мы будем игнорировать warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore', category=DeprecationWarning)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2KMqW4qU7vo",
        "colab_type": "text"
      },
      "source": [
        "для воспроизводимости результатов зафиксируем затравку в рандоме"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUbn78ICOUcY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SEED = 42\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1e9qo_yEOUcd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TRAIN_DIR = Path('../input/simpsons4/train/')\n",
        "TEST_DIR = Path('../input/simpsons4/testset/')\n",
        "\n",
        "#оставим только классы персонажей которых будем распознавать\n",
        "character_list=['abraham_grampa_simpson','apu_nahasapeemapetilon', 'bart_simpson','charles_montgomery_burns', 'chief_wiggum',\\\n",
        "                'comic_book_guy', 'edna_krabappel', 'homer_simpson', 'kent_brockman', 'krusty_the_clown', 'lenny_leonard',\\\n",
        "                'lisa_simpson', 'marge_simpson', 'mayor_quimby', 'milhouse_van_houten', 'moe_szyslak',\n",
        "                'ned_flanders', 'nelson_muntz', 'principal_skinner', 'sideshow_bob']\n",
        "character=[]\n",
        "for name in character_list:\n",
        "    character.extend(list(TRAIN_DIR.rglob(name+'/*.jpg')))\n",
        "train_val_files = sorted(character)\n",
        "test_files = sorted(list(TEST_DIR.rglob('*.jpg')))\n",
        "\n",
        "# разные режимы датасета \n",
        "DATA_MODES = ['train', 'val', 'test']\n",
        "# все изображения будут масштабированы к размеру 224x224 px\n",
        "RESCALE_SIZE = 224\n",
        "# работаем на видеокарте\n",
        "DEVICE = torch.device(\"cuda\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSjgCH2rOUch",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# в исходный класс была добавлена аугментацияданных для тренировочного набора и немного изменен порядок пребразований\n",
        "class SimpsonsDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Датасет с картинками, который паралельно подгружает их из папок\n",
        "    производит скалирование и превращение в торчевые тензоры\n",
        "    \"\"\"\n",
        "    def __init__(self, files, mode):\n",
        "        super().__init__()\n",
        "        # список файлов для загрузки\n",
        "        self.files = sorted(files)\n",
        "        # режим работы\n",
        "        self.mode = mode\n",
        "\n",
        "        if self.mode not in DATA_MODES:\n",
        "            print(f\"{self.mode} is not correct; correct modes: {DATA_MODES}\")\n",
        "            raise NameError\n",
        "\n",
        "        self.len_ = len(self.files)\n",
        "     \n",
        "        self.label_encoder = LabelEncoder()\n",
        "\n",
        "        if self.mode != 'test':\n",
        "            self.labels = [path.parent.name for path in self.files]\n",
        "            self.label_encoder.fit(self.labels)\n",
        "\n",
        "            with open('label_encoder.pkl', 'wb') as le_dump_file:\n",
        "                  pickle.dump(self.label_encoder, le_dump_file)\n",
        "                      \n",
        "    def __len__(self):\n",
        "        return self.len_\n",
        "      \n",
        "    def load_sample(self, file):\n",
        "        image = Image.open(file)\n",
        "        image.load()\n",
        "        return image\n",
        "  \n",
        "    def __getitem__(self, index):\n",
        "        # для преобразования изображений в тензоры PyTorch и нормализации входа\n",
        "        transform=[]\n",
        "        if self.mode == 'train':\n",
        "            transform = transforms.Compose([\n",
        "            transforms.Resize([RESCALE_SIZE, RESCALE_SIZE]),\n",
        "            #аугментация\n",
        "            transforms.RandomRotation(20),\n",
        "            transforms.RandomHorizontalFlip(p=0.5),  \n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ])\n",
        "        else:\n",
        "            transform = transforms.Compose([\n",
        "            transforms.Resize([RESCALE_SIZE, RESCALE_SIZE]),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ])\n",
        "        \n",
        "        x = self.load_sample(self.files[index])\n",
        "        #x = np.array(x / 255, dtype='float32')\n",
        "        x = transform(x)\n",
        "        if self.mode == 'test':\n",
        "            return x\n",
        "        else:\n",
        "            label = self.labels[index]\n",
        "            label_id = self.label_encoder.transform([label])\n",
        "            y = label_id.item()\n",
        "            return x, y\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSEb08t_OUco",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#ничего не менялось\n",
        "def fit_epoch(model, train_loader, criterion, optimizer):\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    processed_data = 0\n",
        "  \n",
        "    for inputs, labels in train_loader:\n",
        "        inputs = inputs.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        preds = torch.argmax(outputs, 1)\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "        processed_data += inputs.size(0)\n",
        "              \n",
        "    train_loss = running_loss / processed_data\n",
        "    train_acc = running_corrects.cpu().numpy() / processed_data\n",
        "    return train_loss, train_acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4p5Fl50OUct",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#ничего не менялось\n",
        "def eval_epoch(model, val_loader, criterion):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    processed_size = 0\n",
        "\n",
        "    for inputs, labels in val_loader:\n",
        "        inputs = inputs.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "\n",
        "        with torch.set_grad_enabled(False):\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            preds = torch.argmax(outputs, 1)\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "        processed_size += inputs.size(0)\n",
        "    val_loss = running_loss / processed_size\n",
        "    val_acc = running_corrects.double() / processed_size\n",
        "    return val_loss, val_acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AThtTujOUcw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#был добавлен планировик для изменения lr и сохранение весов, но на них в каггле не хватает памяти\n",
        "def train(train_files, val_files, model, epochs, batch_size):\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    history = []\n",
        "    log_template = \"\\nEpoch {ep:03d} train_loss: {t_loss:0.4f} \\\n",
        "    val_loss {v_loss:0.4f} train_acc {t_acc:0.4f} val_acc {v_acc:0.4f}\"\n",
        "\n",
        "    with tqdm(desc=\"epoch\", total=epochs) as pbar_outer:\n",
        "        opt = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, 'min')\n",
        "        for epoch in range(epochs):\n",
        "            train_loss, train_acc = fit_epoch(model, train_loader, criterion, opt)\n",
        "            print(\"loss\", train_loss)\n",
        "            \n",
        "            val_loss, val_acc = eval_epoch(model, val_loader, criterion)\n",
        "            history.append((train_loss, train_acc, val_loss, val_acc))\n",
        "            \n",
        "            pbar_outer.update(1)\n",
        "            tqdm.write(log_template.format(ep=epoch+1, t_loss=train_loss,\\\n",
        "                                           v_loss=val_loss, t_acc=train_acc, v_acc=val_acc))\n",
        "            scheduler.step(val_loss)\n",
        "            #torch.save(model.state_dict(), str(model)+str(epoch)+'.pth')\n",
        "            \n",
        "    return history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mHE-j4ZOUc0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#ничего не менялось\n",
        "def predict(model, test_loader):\n",
        "    with torch.no_grad():\n",
        "        logits = []\n",
        "    \n",
        "        for inputs in test_loader:\n",
        "            inputs = inputs.to(DEVICE)\n",
        "            model.eval()\n",
        "            outputs = model(inputs).cpu()\n",
        "            logits.append(outputs)\n",
        "            \n",
        "    probs = nn.functional.softmax(torch.cat(logits), dim=-1).numpy()\n",
        "    return probs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXgcmMrlOUc3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_val_labels = [path.parent.name for path in train_val_files]\n",
        "train_files, val_files = train_test_split(train_val_files, test_size=0.25, \\\n",
        "                                          stratify=train_val_labels, random_state=42,shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOGyX0EdW70q",
        "colab_type": "text"
      },
      "source": [
        "импортируем и переопределуем последние слои у МНОГАМОДЕЛЕЙ чтобы строить ансамбль."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qQhkPk_OUc7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torchvision.models as models\n",
        "resnet18 = models.resnet18(pretrained=True)\n",
        "alexnet = models.alexnet(pretrained=True)\n",
        "squeezenet = models.squeezenet1_0(pretrained=True)\n",
        "vgg16 = models.vgg16(pretrained=True)\n",
        "densenet=models.densenet161(pretrained=True)\n",
        "googlenet = models.googlenet(pretrained=True)\n",
        "shufflenet = models.shufflenet_v2_x1_0(pretrained=True)\n",
        "mobilenet = models.mobilenet_v2(pretrained=True)\n",
        "resnext50_32x4d = models.resnext50_32x4d(pretrained=True)\n",
        "wide_resnet50_2 = models.wide_resnet50_2(pretrained=True)\n",
        "mnasnet = models.mnasnet1_0(pretrained=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcK0wmeyOUdA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_classes = len(np.unique(train_val_labels))\n",
        "\n",
        "resnet18.fc = nn.Linear(resnet18.fc.in_features, n_classes)\n",
        "alexnet.classifier= nn.Sequential(\n",
        "            nn.Dropout(p=0.5, inplace=False),\n",
        "            nn.Linear(in_features=9216, out_features=4096, bias=True),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p=0.5, inplace=False),\n",
        "            nn.Linear(in_features=4096, out_features=4096, bias=True),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(in_features=4096, out_features=n_classes, bias=True)\n",
        "        )       \n",
        "squeezenet.classifier=nn.Sequential(\n",
        "            nn.Dropout(p=0.5, inplace=False),\n",
        "            nn.Conv2d(512, n_classes, kernel_size=(1, 1), stride=(1, 1)),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
        "        )\n",
        "vgg16.classifier =nn.Sequential(\n",
        "            nn.Linear(in_features=25088, out_features=4096, bias=True),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p=0.5, inplace=False),\n",
        "            nn.Linear(in_features=4096, out_features=4096, bias=True),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p=0.5, inplace=False),\n",
        "            nn.Linear(in_features=4096, out_features=n_classes, bias=True)\n",
        "        )\n",
        "densenet.classifier=nn.Linear(in_features=2208, out_features=n_classes, bias=True)\n",
        "googlenet.fc=nn.Linear(in_features=1024, out_features=n_classes, bias=True)\n",
        "shufflenet.fc=nn.Linear(in_features=1024, out_features=n_classes, bias=True)\n",
        "mobilenet.classifier=nn.Sequential(\n",
        "            nn.Dropout(p=0.2, inplace=False),\n",
        "            nn.Linear(in_features=1280, out_features=n_classes, bias=True)\n",
        "        )\n",
        "resnext50_32x4d.fc=nn.Linear(in_features=2048, out_features=n_classes, bias=True)\n",
        "wide_resnet50_2.fc=nn.Linear(in_features=2048, out_features=n_classes, bias=True)\n",
        "mnasnet.classifier=nn.Sequential(\n",
        "            nn.Dropout(p=0.2, inplace=True),\n",
        "            nn.Linear(in_features=1280, out_features=n_classes, bias=True)\n",
        "  )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_WoWj4zOUdE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "resnet18.to(DEVICE)\n",
        "alexnet.to(DEVICE)\n",
        "squeezenet.to(DEVICE)\n",
        "vgg16.to(DEVICE)\n",
        "densenet.to(DEVICE)\n",
        "googlenet.to(DEVICE)\n",
        "shufflenet.to(DEVICE)\n",
        "mobilenet.to(DEVICE)\n",
        "resnext50_32x4d.to(DEVICE)\n",
        "wide_resnet50_2.to(DEVICE)\n",
        "mnasnet.to(DEVICE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wezlf03OUdI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_dataset = SimpsonsDataset(val_files, mode='val')    \n",
        "train_dataset = SimpsonsDataset(train_files, mode='train')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FtAjH2YoOUdN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train(train_dataset, val_dataset, model=resnet18, epochs=10, batch_size=32)\n",
        "train(train_dataset, val_dataset, model=alexnet, epochs=10, batch_size=32)\n",
        "train(train_dataset, val_dataset, model=squeezenet, epochs=10, batch_size=32)\n",
        "train(train_dataset, val_dataset, model=vgg16, epochs=10, batch_size=32)\n",
        "train(train_dataset, val_dataset, model=densenet, epochs=10, batch_size=32)\n",
        "train(train_dataset, val_dataset, model=googlenet, epochs=10, batch_size=32)\n",
        "train(train_dataset, val_dataset, model=shufflenet, epochs=10, batch_size=32)\n",
        "train(train_dataset, val_dataset, model=mobilenet, epochs=10, batch_size=32)\n",
        "train(train_dataset, val_dataset, model=resnext50_32x4d, epochs=10, batch_size=32)\n",
        "train(train_dataset, val_dataset, model=wide_resnet50_2, epochs=10, batch_size=32)\n",
        "train(train_dataset, val_dataset, model=mnasnet, epochs=10, batch_size=32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xHe8ux2XTRD",
        "colab_type": "text"
      },
      "source": [
        "делаем предсказания"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dX1TY6wcOUdS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_dataset = SimpsonsDataset(test_files, mode=\"test\")\n",
        "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=64)\n",
        "\n",
        "\n",
        "resnet18probs = predict(resnet18, test_loader)\n",
        "alexnetprobs = predict(alexnet, test_loader)\n",
        "squeezenetprobs = predict(squeezenet, test_loader)\n",
        "vgg16probs = predict(vgg16, test_loader)\n",
        "densenetprobs = predict(densenet, test_loader)\n",
        "googlenetprobs = predict(googlenet, test_loader)\n",
        "shufflenetprobs = predict(shufflenet, test_loader)\n",
        "mobilenetprobs = predict(mobilenet, test_loader)\n",
        "resnext50_32x4dprobs = predict(resnext50_32x4d, test_loader)\n",
        "wide_resnet50_2probs = predict(wide_resnet50_2, test_loader)\n",
        "mnasnetprobs = predict(mnasnet, test_loader)\n",
        "\n",
        "probs1=resnet18probs + alexnetprobs + squeezenetprobs + vgg16probs + densenetprobs +  googlenetprobs +\\\n",
        "        shufflenetprobs + mobilenetprobs + resnext50_32x4dprobs + wide_resnet50_2probs + mnasnetprobs\n",
        "probs2=np.maximum.reduce([resnet18probs , alexnetprobs , squeezenetprobs , vgg16probs , densenetprobs , googlenetprobs ,\\\n",
        "        shufflenetprobs , mobilenetprobs , resnext50_32x4dprobs , wide_resnet50_2probs , mnasnetprobs])\n",
        "from scipy import stats\n",
        "probs3=stats.mode([np.argmax(resnet18probs, axis=1), np.argmax(alexnetprobs, axis=1), np.argmax(squeezenetprobs, axis=1), \\\n",
        "        np.argmax(vgg16probs, axis=1), np.argmax(densenetprobs, axis=1), \\\n",
        "        np.argmax(googlenetprobs, axis=1), np.argmax(shufflenetprobs, axis=1), np.argmax(mobilenetprobs, axis=1), \\\n",
        "        np.argmax(resnext50_32x4dprobs, axis=1), np.argmax(wide_resnet50_2probs, axis=1), np.argmax(mnasnetprobs, axis=1)])[0]\n",
        "\n",
        "label_encoder = pickle.load(open(\"label_encoder.pkl\", 'rb'))\n",
        "\n",
        "preds1 = label_encoder.inverse_transform(np.argmax(probs1, axis=1))\n",
        "preds2 = label_encoder.inverse_transform(np.argmax(probs2, axis=1))\n",
        "preds3 = label_encoder.inverse_transform(probs3[0])\n",
        "\n",
        "test_filenames = [path.name for path in test_dataset.files]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZ_g3qWtOUdW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "#my_submit = pd.read_csv(\"gdrive/My Drive/simpsons/data/labels.csv\")\n",
        "my_submit1 = pd.DataFrame({'Id': test_filenames, 'Expected': preds1})\n",
        "my_submit2 = pd.DataFrame({'Id': test_filenames, 'Expected': preds2})\n",
        "my_submit3 = pd.DataFrame({'Id': test_filenames, 'Expected': preds3})\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyy7oRbnXbdY",
        "colab_type": "text"
      },
      "source": [
        "Все сохраняем"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7GQoQhlOUda",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "my_submit1.to_csv('ansumble1.csv', index=False)\n",
        "my_submit2.to_csv('ansumble2.csv', index=False)\n",
        "my_submit3.to_csv('ansumble3.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EVHrm-lOUde",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.DataFrame(resnet18probs).to_csv('resnet18probs.csv', index=False)\n",
        "pd.DataFrame(alexnetprobs).to_csv('alexnetprobs.csv', index=False)\n",
        "pd.DataFrame(squeezenetprobs).to_csv('squeezenetprobs.csv', index=False)\n",
        "pd.DataFrame(vgg16probs).to_csv('vgg16probs.csv', index=False)\n",
        "pd.DataFrame(densenetprobs).to_csv('densenetprobs.csv', index=False)\n",
        "pd.DataFrame(googlenetprobs).to_csv('googlenetprobs.csv', index=False)\n",
        "pd.DataFrame(shufflenetprobs).to_csv('shufflenetprobs.csv', index=False)\n",
        "pd.DataFrame(mobilenetprobs).to_csv('mobilenetprobs.csv', index=False)\n",
        "pd.DataFrame(resnext50_32x4dprobs).to_csv('resnext50_32x4dprobs.csv', index=False)\n",
        "pd.DataFrame(wide_resnet50_2probs).to_csv('wide_resnet50_2probs.csv', index=False)\n",
        "pd.DataFrame(mnasnetprobs).to_csv('mnasnetprobs.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeSiCN08Xl43",
        "colab_type": "text"
      },
      "source": [
        "Что было вообще"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyWMtwNaXpgr",
        "colab_type": "text"
      },
      "source": [
        "обучанеи своей нейронной сети\n",
        "Public Score: 0.96632\n",
        "\n",
        "дообучение моделей с заморозкой слоев\n",
        "Public Score: 0.76094\n",
        "\n",
        "дообучение моделеи ResNet18 с без заморозки слоев\n",
        "Public Score: 0.98653\n",
        "\n",
        "\n",
        "ансамбль с усреднением веоятности (my_submit1)\n",
        "Public Score: 0.99663\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dfErkd5XplM",
        "colab_type": "text"
      },
      "source": [
        "# и вишенка на торте - Public Score: 1.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5bZ_umEZEWe",
        "colab_type": "text"
      },
      "source": [
        "в процессе обучения модели  давали разный скор на трейне и валидации. после отправки трех сабмитов из данного нотебука было выясненно, что в лучшем случае не корректно распознается одна картинка на паблике. Также были вручную отсмотрены случаи, где по результатам голосования (сабмит3) решение принято меньше,чем половиной голосов. В итоге было отобрано и в ручную определен класс для двух картинок. На основании внутренних суждений, скора при обучении и ранее полученных резульатах отправки сабмита была сделана следуюая финальная модель "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McSLyUpSYpRj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "probs=resnet18probs + 0.5*alexnetprobs + squeezenetprobs + 5*vgg16probs + 3*densenetprobs +  4*googlenetprobs +\\\n",
        "        shufflenetprobs + mobilenetprobs + 0.5*resnext50_32x4dprobs + wide_resnet50_2probs + mnasnetprobs\n",
        "preds = label_encoder.inverse_transform(np.argmax(probs.values, axis=1))\n",
        "my_submit = pd.DataFrame({'Id': test_filenames, 'Expected': preds})\n",
        "my_submit.to_csv('total.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}